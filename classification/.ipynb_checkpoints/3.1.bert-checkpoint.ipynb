{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489da2f7-b800-4a1d-b9fc-7826c966eb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 13:29:19.847039: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-30 13:29:20.413825: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-30 13:29:20.413868: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-30 13:29:20.413899: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-30 13:29:20.448064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 13:29:26.296721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fc031c4e2b425bbe7ce86f30ded126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c284fe68b3c546338b0ed1bc034e1a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ada4d86c07c450e9f88d7df2faade63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/150 02:47 < 00:12, 0.83 it/s, Epoch 23.17/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score Class 0</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Score Class 1</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Score Class 2</th>\n",
       "      <th>Precision Class 2</th>\n",
       "      <th>Recall Class 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.016426</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.028372</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.043193</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.066606</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.120517</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.173663</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.185951</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.175926</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.227700</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.284613</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.302755</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.473424</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.446046</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.504072</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.567640</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.582730</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.619609</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.611266</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.636154</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.690318</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.684701</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.652444</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.661278</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.674328</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using the Excerpt for the training and using the [top n parapgraphs] for the validation and test data\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "import scipy.special\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Read the gold_standard.xlsx file and create a dictionary mapping DocID to IDEA CAREER 3 and Excerpt\n",
    "df = pd.read_excel('gold_standard.xlsx')\n",
    "\n",
    "# Read the highest_scoring_paragraph.csv file and create a dictionary mapping DocID to Paragraph\n",
    "paragraph_df = pd.read_csv('highest_scoring_paragraphs.csv')\n",
    "docid_to_paragraph = defaultdict(list)\n",
    "for _, row in paragraph_df.iterrows():\n",
    "    doc_id_str = str(row['DocID'])\n",
    "    if len(doc_id_str) == 6:\n",
    "        doc_id_str = '0' + doc_id_str\n",
    "    docid_to_paragraph[doc_id_str].append(row['Paragraph'])\n",
    "\n",
    "docid_to_label = {}\n",
    "docid_to_excerpt = {}\n",
    "for _, row in df.iterrows():\n",
    "    doc_ids = str(row['DocID']).split(', ')\n",
    "    for doc_id in doc_ids:\n",
    "        docid_to_label[str(doc_id.strip())] = row['IDEA CAREER 3']\n",
    "        docid_to_excerpt[str(doc_id.strip())] = row['Excerpt']\n",
    "\n",
    "# Initialize an empty list to store batch sentences\n",
    "batch_sentences = []\n",
    "\n",
    "results_list_temp = []\n",
    "\n",
    "# Path to the folder containing XML files\n",
    "folder_path = \"gold_standard_files\"\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".xml\"):\n",
    "        # Extract DocID from the filename\n",
    "        doc_id = str(file_name.split(\".\")[0])\n",
    "\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(os.path.join(folder_path, file_name))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract the title\n",
    "        title = root.find(\".//title\").text\n",
    "\n",
    "        # Extract paragraphs and concatenate with the title\n",
    "        article_content = [title]\n",
    "        for paragraph in root.findall(\".//p\"):\n",
    "            article_content.append(paragraph.text)\n",
    "        \n",
    "        # Convert the article content into one string\n",
    "        combined_text = \" \".join(article_content)\n",
    "\n",
    "        # Fetch the corresponding label using the DocID and adjust the label\n",
    "        label = docid_to_label.get(doc_id, None)\n",
    "        if label is not None:\n",
    "            adjusted_label = label - 1  # Adjust the label\n",
    "            batch_sentences.append({'doc_id': doc_id, 'label': adjusted_label, 'text': combined_text})\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Global variable to store logits\n",
    "global_logits = None\n",
    "\n",
    "def compute_metrics_with_logits(eval_pred):\n",
    "    global global_logits\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Store the logits for later use\n",
    "    global_logits = logits\n",
    "    \n",
    "    # Compute metrics for each class separately\n",
    "    f1_scores = f1_score(labels, predictions, average=None, zero_division=0).tolist()  # Convert to list\n",
    "    precisions = precision_score(labels, predictions, average=None, zero_division=0).tolist()  # Convert to list\n",
    "    recalls = recall_score(labels, predictions, average=None, zero_division=0).tolist()  # Convert to list\n",
    "    \n",
    "    # Update the dictionaries with the metrics for each class\n",
    "    for i in range(len(f1_scores)):\n",
    "        class_metric_sums[i][\"f1_score\"] += f1_scores[i]\n",
    "        class_metric_sums[i][\"precision\"] += precisions[i]\n",
    "        class_metric_sums[i][\"recall\"] += recalls[i]\n",
    "        class_counts[i] += 1  # Assuming there is at least one instance for each class in every fold\n",
    "\n",
    "    # Calculate the average metric results\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "\n",
    "    metrics = {\n",
    "        \"f1_score\": avg_f1_score,  # Now the average value\n",
    "        \"precision\": avg_precision,  # Now the average value\n",
    "        \"recall\": avg_recall,  # Now the average value\n",
    "        \"accuracy\": accuracy_score(labels, predictions)\n",
    "    }\n",
    "    \n",
    "    # Store metrics for each class in the dictionary\n",
    "    for i in range(len(f1_scores)):\n",
    "        metrics[f\"f1_score_class_{i}\"] = f1_scores[i]\n",
    "        metrics[f\"precision_class_{i}\"] = precisions[i]\n",
    "        metrics[f\"recall_class_{i}\"] = recalls[i]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Folder containing the 5-fold CSV files\n",
    "fold_folder = \"doc_id_5_fold\"\n",
    "\n",
    "# Initialize lists to store metric values for each fold\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "class_metric_sums = defaultdict(lambda: defaultdict(float))\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "class_metrics = {\n",
    "    0: {\"precision\": [], \"recall\": [], \"f1-score\": []},\n",
    "    1: {\"precision\": [], \"recall\": [], \"f1-score\": []},\n",
    "    2: {\"precision\": [], \"recall\": [], \"f1-score\": []}\n",
    "}\n",
    "\n",
    "# Iterate over each CSV file in the fold folder\n",
    "for fold_file in os.listdir(fold_folder):\n",
    "    if fold_file.endswith(\".csv\"):\n",
    "        \n",
    "        # Model, TrainingArguments, and Trainer initialization\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"test_trainer\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=1e-5,\n",
    "            per_device_train_batch_size=32,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=25,\n",
    "            #warmup_steps=50,\n",
    "            #weight_decay=0.01,  \n",
    "            logging_dir='./logs',\n",
    "            logging_steps=500,\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=3,\n",
    "        )\n",
    "\n",
    "        with open(os.path.join(fold_folder, fold_file), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            # Split the lines based on empty lines\n",
    "            splits = [i for i, line in enumerate(lines) if not line.strip()]\n",
    "            train_ids = [str(line.strip()) for line in lines[:splits[0]]]\n",
    "            val_ids = [str(line.strip()) for line in lines[splits[0]+1:splits[1]]]\n",
    "            test_ids = [str(line.strip()) for line in lines[splits[1]+1:]]\n",
    "\n",
    "        #train_data = []\n",
    "        #for doc_id in train_ids:\n",
    "            #excerpt = docid_to_excerpt.get(doc_id, None)\n",
    "            #label = docid_to_label.get(doc_id, None)\n",
    "            #if label is not None and excerpt is not None:\n",
    "                #adjusted_label = label - 1  # Adjust the label\n",
    "                #train_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': excerpt})\n",
    "\n",
    "        train_data = []\n",
    "        for doc_id in train_ids:\n",
    "            paragraphs = docid_to_paragraph.get(doc_id, [])\n",
    "            for paragraph in paragraphs:\n",
    "                label = docid_to_label.get(doc_id, None)\n",
    "                if label is not None and paragraph is not None:\n",
    "                    adjusted_label = label - 1  # Adjust the label\n",
    "                    train_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': paragraph})\n",
    "\n",
    "        #val_data = []\n",
    "        #for doc_id in val_ids:\n",
    "            #excerpt = docid_to_excerpt.get(doc_id, None)\n",
    "            #label = docid_to_label.get(doc_id, None)\n",
    "            #if label is not None and excerpt is not None:\n",
    "                #adjusted_label = label - 1  # Adjust the label\n",
    "                #val_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': excerpt})\n",
    "\n",
    "        val_data = []\n",
    "        for doc_id in val_ids:\n",
    "            paragraphs = docid_to_paragraph.get(doc_id, [])\n",
    "            for paragraph in paragraphs:\n",
    "                label = docid_to_label.get(doc_id, None)\n",
    "                if label is not None and paragraph is not None:\n",
    "                    adjusted_label = label - 1  # Adjust the label\n",
    "                    val_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': paragraph})\n",
    "                    \n",
    "        #test_data = []\n",
    "        #for doc_id in test_ids:\n",
    "            #excerpt = docid_to_excerpt.get(doc_id, None)\n",
    "            #label = docid_to_label.get(doc_id, None)\n",
    "            #if label is not None and excerpt is not None:\n",
    "                #adjusted_label = label - 1  # Adjust the label\n",
    "                #test_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': excerpt})\n",
    "\n",
    "        test_data = []\n",
    "        for doc_id in test_ids:\n",
    "            paragraphs = docid_to_paragraph.get(doc_id, [])\n",
    "            for paragraph in paragraphs:\n",
    "                label = docid_to_label.get(doc_id, None)\n",
    "                if label is not None and paragraph is not None:\n",
    "                    adjusted_label = label - 1  # Adjust the label\n",
    "                    test_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': paragraph})\n",
    "\n",
    "        # Convert data to datasets\n",
    "        train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "        val_dataset = Dataset.from_pandas(pd.DataFrame(val_data))\n",
    "        test_dataset = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "        # Tokenize datasets\n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "        val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "        test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Define a search space\n",
    "            #learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "            #num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
    "            #per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32])\n",
    "            #weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-1, log=True)\n",
    "            #warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 300)\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"test_trainer\",\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                #learning_rate=learning_rate,\n",
    "                #num_train_epochs=num_train_epochs,\n",
    "                #per_device_train_batch_size=per_device_train_batch_size,\n",
    "                #weight_decay=weight_decay,\n",
    "                #warmup_steps=warmup_steps,\n",
    "                save_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "            )\n",
    "\n",
    "            # Initialize the Trainer with the hyperparameters\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=compute_metrics_with_logits\n",
    "            )\n",
    "\n",
    "            # Train the model and evaluate it on the validation set\n",
    "            trainer.train()\n",
    "            eval_results = trainer.evaluate()\n",
    "\n",
    "            # Return the metric that you want to optimize\n",
    "            return eval_results[\"eval_loss\"]\n",
    "        \n",
    "        # Create a study object and optimize the objective function\n",
    "        # study = optuna.create_study(direction=\"minimize\")\n",
    "        # study.optimize(objective, n_trials=10)\n",
    "        \n",
    "        # Get the best hyperparameters\n",
    "        # best_params = study.best_params\n",
    "        # print(f\"Best hyperparameters for fold {fold_file}: {best_params}\")\n",
    "\n",
    "        trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metrics_with_logits)\n",
    "        trainer.train()\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(trainer.state.best_model_checkpoint)\n",
    "        best_trainer = Trainer(model=best_model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metrics_with_logits)\n",
    "        eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        logits = global_logits\n",
    "        labels = eval_results.pop('labels', None) if 'labels' in eval_results else None\n",
    "        \n",
    "        # Convert logits to numpy array with float data type\n",
    "        logits = np.array(logits, dtype=float)\n",
    "\n",
    "        probabilities = scipy.special.softmax(logits, axis=1)\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "\n",
    "        print(\"Results for this fold:\")\n",
    "        for key, value in eval_results.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Convert logits to numpy array with float data type\n",
    "        logits = np.array(logits, dtype=float)\n",
    "\n",
    "        # Now compute the softmax\n",
    "        probabilities = scipy.special.softmax(logits, axis=1)\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "\n",
    "        # Updated logic for selecting the highest confidence prediction for each DocID\n",
    "        docid_to_max_conf = defaultdict(float)\n",
    "        docid_to_pred = {}\n",
    "\n",
    "        docid_to_max_conf = {}\n",
    "        docid_to_pred = {}\n",
    "        \n",
    "        # Temporary code\n",
    "        #for doc_id, value, pred in zip(test_dataset['doc_id'], confidence_scores, np.argmax(probabilities, axis=1)):\n",
    "            #true_label = docid_to_label.get(doc_id, None)  # Fetching the true label using doc_id\n",
    "            #if true_label is not None:  # Ensure that the true label exists for the given doc_id\n",
    "                #true_label -= 1  # Adjusting the label\n",
    "                #results_list_temp.append({\n",
    "                    #'DocID': doc_id,\n",
    "                    #'True Label': true_label,\n",
    "                    #'Predicted Label': pred,\n",
    "                    #'Confidence Score': value\n",
    "                #})\n",
    "\n",
    "        for i, (doc_id, value, pred) in enumerate(zip(test_dataset['doc_id'], confidence_scores, np.argmax(probabilities, axis=1))):\n",
    "            true_label = docid_to_label.get(doc_id, None)  # Fetching the true label using doc_id\n",
    "            if true_label is not None:  # Ensure that the true label exists for the given doc_id\n",
    "                true_label -= 1  # Adjusting the label\n",
    "                if doc_id in docid_to_max_conf:\n",
    "                    # Check if True Label and Predicted Label are not the same\n",
    "                    if true_label != pred:\n",
    "                        print('Document ID: ', doc_id, ', True Label: ', true_label, ', Predicted Label: ', pred)\n",
    "                        for j, class_prob in enumerate(probabilities[i]):\n",
    "                            print(f\"  Class {j} Confidence Score: {class_prob:.4f}\")\n",
    "                        print()\n",
    "                    if value > docid_to_max_conf[doc_id]:\n",
    "                        # Update the existing entry in results_list with the higher value\n",
    "                        for result in results_list:\n",
    "                            if result['DocID'] == doc_id:\n",
    "                                result['Predicted Label'] = pred\n",
    "                                result['Confidence Score'] = value\n",
    "                        docid_to_max_conf[doc_id] = value\n",
    "                        docid_to_pred[doc_id] = pred\n",
    "                else:\n",
    "                    # If the doc_id is not in the results_list, append it directly\n",
    "                    results_list.append({\n",
    "                        'DocID': doc_id,\n",
    "                        'True Label': true_label,\n",
    "                        'Predicted Label': pred,\n",
    "                        'Confidence Score': value\n",
    "                    })\n",
    "                    docid_to_max_conf[doc_id] = value\n",
    "                    docid_to_pred[doc_id] = pred\n",
    "        \n",
    "        # Temporary code\n",
    "        if 'Unknown' in test_ids:\n",
    "            test_ids.remove('Unknown')\n",
    "\n",
    "        final_preds = [docid_to_pred[doc_id] for doc_id in test_ids]\n",
    "        final_true_labels = [docid_to_label[doc_id] - 1 for doc_id in test_ids]\n",
    "\n",
    "        # Compute the classification report\n",
    "        report = classification_report(final_true_labels, final_preds, zero_division=0)\n",
    "        print(\"Classification Report for Fold:\", fold_file)\n",
    "        print(report)\n",
    "        report_dict = classification_report(final_true_labels, final_preds, output_dict=True, zero_division=0)\n",
    "\n",
    "        for class_label, metrics in report_dict.items():\n",
    "            if class_label.isdigit():  # Check if the key represents a class label\n",
    "                class_metrics[int(class_label)][\"precision\"].append(metrics[\"precision\"])\n",
    "                class_metrics[int(class_label)][\"recall\"].append(metrics[\"recall\"])\n",
    "                class_metrics[int(class_label)][\"f1-score\"].append(metrics[\"f1-score\"])\n",
    "        \n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(final_true_labels, final_preds)\n",
    "\n",
    "        # Display the confusion matrix using Seaborn's heatmap\n",
    "        plt.figure(figsize=(10,7))\n",
    "        sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix for Fold: ' + fold_file)\n",
    "        plt.show()\n",
    "\n",
    "        results = {}  # Initialize the results dictionary\n",
    "        results[\"f1_score\"] = f1_score(final_true_labels, final_preds, average='weighted', zero_division=0)\n",
    "        results[\"precision\"] = precision_score(final_true_labels, final_preds, average='weighted', zero_division=0)\n",
    "        results[\"recall\"] = recall_score(final_true_labels, final_preds, average='weighted', zero_division=0)\n",
    "        results[\"accuracy\"] = accuracy_score(final_true_labels, final_preds)\n",
    "\n",
    "        # Append metric values for this fold\n",
    "        f1_scores.append(results[\"f1_score\"])\n",
    "        precisions.append(results[\"precision\"])\n",
    "        recalls.append(results[\"recall\"])\n",
    "        accuracies.append(results[\"accuracy\"])\n",
    "\n",
    "        print(results)\n",
    "\n",
    "# Calculate average metric values\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "print(\"Average F1-score:\", avg_f1_score)\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "\n",
    "# Calculate and print the average metrics for each class\n",
    "print(\"Average Metrics Per Class:\")\n",
    "for class_label, metrics in class_metrics.items():\n",
    "    avg_precision = np.mean(metrics[\"precision\"])\n",
    "    avg_recall = np.mean(metrics[\"recall\"])\n",
    "    avg_f1 = np.mean(metrics[\"f1-score\"])\n",
    "    print(f\"Class {class_label} - Precision: {avg_precision}, Recall: {avg_recall}, F1-score: {avg_f1}\")\n",
    "    \n",
    "print(\"Final Results:\")\n",
    "for result in results_list:\n",
    "    print(f\"DocID: {result['DocID']}, True Label: {result['True Label']}, Predicted Label: {result['Predicted Label']}, Confidence Score: {result['Confidence Score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d70906-624a-4148-81b6-e19b5b0b5a2f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        #train_data = []\n",
    "        #for doc_id in train_ids:\n",
    "            #paragraphs = docid_to_paragraph.get(doc_id, [])\n",
    "            #for paragraph in paragraphs:\n",
    "                #label = docid_to_label.get(doc_id, None)\n",
    "                #if label is not None and paragraph is not None:\n",
    "                    #adjusted_label = label - 1  # Adjust the label\n",
    "                    #train_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': paragraph})\n",
    "\n",
    "        train_data = []\n",
    "        for doc_id in train_ids:\n",
    "            excerpt = docid_to_excerpt.get(doc_id, None)\n",
    "            label = docid_to_label.get(doc_id, None)\n",
    "            if label is not None and excerpt is not None:\n",
    "                adjusted_label = label - 1  # Adjust the label\n",
    "                train_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': excerpt})\n",
    "\n",
    "        #val_data = []\n",
    "        #for doc_id in val_ids:\n",
    "            #paragraphs = docid_to_paragraph.get(doc_id, [])\n",
    "            #for paragraph in paragraphs:\n",
    "                #label = docid_to_label.get(doc_id, None)\n",
    "                #if label is not None and paragraph is not None:\n",
    "                    #adjusted_label = label - 1  # Adjust the label\n",
    "                    #val_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': paragraph})\n",
    "\n",
    "        val_data = []\n",
    "        for doc_id in val_ids:\n",
    "            excerpt = docid_to_excerpt.get(doc_id, None)\n",
    "            label = docid_to_label.get(doc_id, None)\n",
    "            if label is not None and excerpt is not None:\n",
    "                adjusted_label = label - 1  # Adjust the label\n",
    "                val_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': excerpt})\n",
    "\n",
    "        #test_data = []\n",
    "        #for doc_id in test_ids:\n",
    "            #paragraphs = docid_to_paragraph.get(doc_id, [])\n",
    "            #for paragraph in paragraphs:\n",
    "                #label = docid_to_label.get(doc_id, None)\n",
    "                #if label is not None and paragraph is not None:\n",
    "                    #adjusted_label = label - 1  # Adjust the label\n",
    "                    #test_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': paragraph})\n",
    "\n",
    "        test_data = []\n",
    "        for doc_id in test_ids:\n",
    "            excerpt = docid_to_excerpt.get(doc_id, None)\n",
    "            label = docid_to_label.get(doc_id, None)\n",
    "            if label is not None and excerpt is not None:\n",
    "                adjusted_label = label - 1  # Adjust the label\n",
    "                test_data.append({'doc_id': doc_id, 'label': adjusted_label, 'text': excerpt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63ea37-a59a-44ef-b909-57030a36aa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
